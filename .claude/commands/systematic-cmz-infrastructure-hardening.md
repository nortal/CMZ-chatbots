# Systematic CMZ Infrastructure Hardening

**Purpose**: Permanently resolve recurring development workflow issues through systematic infrastructure improvements, automation, and template fixes

## Context
Analysis of CMZ-chatbots repository histories reveals 7 major recurring issues that consistently block productivity and require repeated manual fixes. This prompt addresses these systemically through infrastructure hardening, automation, and architectural improvements.

## Sequential Reasoning Approach

Use MCP Sequential Thinking to systematically harden the CMZ infrastructure:

### Phase 1: Issue Analysis & Root Cause Identification
**Use Sequential Reasoning to:**
1. **Audit Current State**: Document all recurring issues with frequency and impact metrics
2. **Map Dependencies**: Identify relationships between issues (e.g., OpenAPI template problems trigger import errors)
3. **Prioritize by Impact**: Rank issues by time cost and blocking severity
4. **Identify System Patterns**: Find underlying architectural mismatches causing multiple symptoms
5. **Plan Systematic Solutions**: Design fixes that prevent recurrence rather than treating symptoms

**Key Questions for Sequential Analysis:**
- Which issues are symptoms vs root causes?
- What architectural decisions create recurring friction?
- Which fixes will prevent multiple issue categories?
- How can we automate detection and prevention?
- What validation can catch issues before they impact development?

### Phase 2: Infrastructure Hardening Implementation
**Implementation Order (Follow Exactly):**

#### Step 1: OpenAPI Template Architecture Fix (HIGHEST IMPACT)
**Problem**: Generated controllers incompatible with hexagonal architecture
**Solution**: Permanent template modification

```bash
# 1. Create custom controller template that matches architecture
mkdir -p backend/api/templates/python-flask
cat > backend/api/templates/python-flask/controller.mustache << 'EOF'
# Template that works with hexagonal architecture generic routing
try:
    from {{package}}.impl import handlers
    impl_function = handlers.handle_
    result = impl_function({{#hasParams}}{{#allParams}}{{paramName}}{{#hasMore}}, {{/hasMore}}{{/allParams}}{{/hasParams}})
    return result if isinstance(result, tuple) else (result, 200)
except NotImplementedError as e:
    from {{package}}.models.error import Error
    error_obj = Error(code="not_implemented", message=f"Controller {{operationId}} implementation not found: {str(e)}")
    return error_obj, 501
EOF

# 2. Update Makefile to use custom templates
sed -i 's/OPENAPI_GEN_OPTS := .*/OPENAPI_GEN_OPTS := --template-dir backend\/api\/templates/' Makefile

# 3. Create post-generation automation script
cat > scripts/post_openapi_generation.py << 'EOF'
#!/usr/bin/env python3
"""
Automated post-OpenAPI generation fixes for common issues
"""
import os
import re
import subprocess
from pathlib import Path

def fix_import_paths():
    """Fix common import path issues in generated code"""
    # Fix controller imports
    for file in Path("backend/api/src/main/python/openapi_server/controllers").glob("*.py"):
        content = file.read_text()
        content = re.sub(r'from openapi_server\.controllers\.impl', 'from openapi_server.impl', content)
        content = re.sub(r'from openapi_server\.controllers\.models', 'from openapi_server.models', content)
        content = re.sub(r'from  import encoder', 'from . import encoder', content)
        file.write_text(content)

    # Fix main module imports
    main_file = Path("backend/api/src/main/python/openapi_server/__main__.py")
    if main_file.exists():
        content = main_file.read_text()
        content = re.sub(r'from  import encoder', 'from . import encoder', content)
        main_file.write_text(content)

def validate_generation():
    """Validate that generation completed successfully"""
    try:
        result = subprocess.run([
            "python", "-c", "from openapi_server.models import *; from openapi_server.impl import handlers"
        ], cwd="backend/api/src/main/python", capture_output=True, text=True)

        if result.returncode == 0:
            print("âœ… OpenAPI generation validation passed")
            return True
        else:
            print(f"âŒ OpenAPI generation validation failed: {result.stderr}")
            return False
    except Exception as e:
        print(f"âŒ Validation error: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ”§ Running post-OpenAPI generation fixes...")
    fix_import_paths()
    if validate_generation():
        print("âœ… All fixes applied successfully")
    else:
        print("âŒ Manual intervention required")
        exit(1)
EOF
chmod +x scripts/post_openapi_generation.py

# 4. Update Makefile to run post-generation fixes
sed -i '/generate-api:/a\\t@echo "Running post-generation fixes..."\n\tpython3 scripts/post_openapi_generation.py' Makefile
```

#### Step 2: Environment Startup Automation (PRODUCTIVITY IMPACT)
**Problem**: Services not running causes failed validations
**Solution**: Automated environment setup with health checks

```bash
# 1. Create comprehensive startup script
cat > scripts/start_development_environment.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸš€ Starting CMZ Development Environment..."

# Function to check if port is in use
check_port() {
    local port=$1
    if lsof -i:$port >/dev/null 2>&1; then
        echo "âš ï¸  Port $port is in use"
        lsof -i:$port
        read -p "Kill existing process? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            lsof -ti:$port | xargs kill -9 2>/dev/null || true
            sleep 2
        else
            echo "âŒ Cannot start services with port conflict"
            exit 1
        fi
    fi
}

# Function to wait for service health
wait_for_health() {
    local url=$1
    local service=$2
    local max_attempts=30
    local attempt=1

    echo "â³ Waiting for $service to be healthy..."
    while [ $attempt -le $max_attempts ]; do
        if curl -f "$url" >/dev/null 2>&1; then
            echo "âœ… $service is healthy"
            return 0
        fi
        echo "   Attempt $attempt/$max_attempts: $service not ready..."
        sleep 2
        ((attempt++))
    done

    echo "âŒ $service failed to start within timeout"
    return 1
}

# Check and kill conflicting ports
check_port 3000
check_port 3001
check_port 8080

# Verify AWS credentials
if ! aws sts get-caller-identity --profile cmz >/dev/null 2>&1; then
    echo "âŒ AWS credentials not configured for CMZ profile"
    echo "Run: aws configure --profile cmz"
    exit 1
fi

# Start backend API
echo "ğŸ”§ Starting backend API..."
cd "$(dirname "$0")/.."
make generate-api && make build-api && make run-api &
BACKEND_PID=$!

# Wait for backend health
if wait_for_health "http://localhost:8080/health" "Backend API"; then
    echo "âœ… Backend API started successfully (PID: $BACKEND_PID)"
else
    kill $BACKEND_PID 2>/dev/null || true
    exit 1
fi

# Start frontend (detect which port to use)
echo "ğŸ¨ Starting frontend..."
cd frontend
if [ -f "package.json" ]; then
    npm run dev &
    FRONTEND_PID=$!

    # Try both common ports
    if wait_for_health "http://localhost:3000" "Frontend (port 3000)"; then
        FRONTEND_URL="http://localhost:3000"
    elif wait_for_health "http://localhost:3001" "Frontend (port 3001)"; then
        FRONTEND_URL="http://localhost:3001"
    else
        echo "âŒ Frontend failed to start"
        kill $BACKEND_PID $FRONTEND_PID 2>/dev/null || true
        exit 1
    fi

    echo "âœ… Frontend started successfully at $FRONTEND_URL (PID: $FRONTEND_PID)"
else
    echo "âš ï¸  No frontend package.json found, skipping frontend startup"
fi

# Verify full system health
echo "ğŸ” Running system health check..."
if curl -f http://localhost:8080/health >/dev/null 2>&1; then
    echo "âœ… Backend API: HEALTHY"
else
    echo "âŒ Backend API: UNHEALTHY"
fi

if [ -n "$FRONTEND_URL" ] && curl -f "$FRONTEND_URL" >/dev/null 2>&1; then
    echo "âœ… Frontend: HEALTHY"
else
    echo "âŒ Frontend: UNHEALTHY"
fi

# Verify database connectivity
if aws dynamodb list-tables --region us-west-2 --profile cmz >/dev/null 2>&1; then
    echo "âœ… DynamoDB: CONNECTED"
else
    echo "âŒ DynamoDB: CONNECTION FAILED"
fi

echo "ğŸ‰ Development environment startup complete!"
echo "   Backend API: http://localhost:8080"
echo "   Frontend: $FRONTEND_URL"
echo "   Logs: make logs-api"

# Save PIDs for cleanup
echo "$BACKEND_PID" > .backend.pid
[ -n "$FRONTEND_PID" ] && echo "$FRONTEND_PID" > .frontend.pid
EOF
chmod +x scripts/start_development_environment.sh

# 2. Create cleanup script
cat > scripts/stop_development_environment.sh << 'EOF'
#!/bin/bash
echo "ğŸ›‘ Stopping CMZ Development Environment..."

# Kill saved PIDs
if [ -f ".backend.pid" ]; then
    kill $(cat .backend.pid) 2>/dev/null || true
    rm .backend.pid
fi

if [ -f ".frontend.pid" ]; then
    kill $(cat .frontend.pid) 2>/dev/null || true
    rm .frontend.pid
fi

# Kill by port
lsof -ti:3000,3001,8080 | xargs kill -9 2>/dev/null || true

# Stop Docker containers
make stop-api 2>/dev/null || true

echo "âœ… Development environment stopped"
EOF
chmod +x scripts/stop_development_environment.sh

# 3. Add Makefile targets
cat >> Makefile << 'EOF'

# Development environment management
start-dev: ## Start complete development environment
	@scripts/start_development_environment.sh

stop-dev: ## Stop complete development environment
	@scripts/stop_development_environment.sh

health-check: ## Check system health
	@echo "ğŸ” System Health Check"
	@curl -f http://localhost:8080/health && echo "âœ… Backend: OK" || echo "âŒ Backend: FAIL"
	@curl -f http://localhost:3000 >/dev/null 2>&1 && echo "âœ… Frontend (3000): OK" || echo "âŒ Frontend (3000): FAIL"
	@curl -f http://localhost:3001 >/dev/null 2>&1 && echo "âœ… Frontend (3001): OK" || echo "âŒ Frontend (3001): FAIL"
	@aws dynamodb list-tables --region us-west-2 --profile cmz >/dev/null 2>&1 && echo "âœ… DynamoDB: OK" || echo "âŒ DynamoDB: FAIL"
EOF
```

#### Step 3: Git Workflow Enforcement (PROCESS IMPACT)
**Problem**: Wrong branch targeting and workflow violations
**Solution**: Automated checks and templates

```bash
# 1. Create pre-commit hook for branch validation
mkdir -p .git/hooks
cat > .git/hooks/pre-commit << 'EOF'
#!/bin/bash
# CMZ Git Workflow Validation

current_branch=$(git branch --show-current)

# Prevent direct commits to protected branches
if [[ "$current_branch" == "main" || "$current_branch" == "master" || "$current_branch" == "dev" ]]; then
    echo "âŒ Direct commits to $current_branch are not allowed"
    echo "   Create a feature branch: git checkout -b feature/your-feature-name"
    exit 1
fi

# Validate feature branch naming
if [[ ! "$current_branch" =~ ^(feature|hotfix|bugfix)/ ]]; then
    echo "âš ï¸  Branch name '$current_branch' doesn't follow naming convention"
    echo "   Expected: feature/description, hotfix/issue, or bugfix/issue"
    read -p "Continue anyway? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

# Check for OpenAPI spec changes
if git diff --cached --name-only | grep -q "openapi_spec.yaml"; then
    echo "âš ï¸  OpenAPI spec changed - remember to validate handlers after regeneration"
    echo "   Run: make generate-api && python scripts/post_openapi_generation.py"
fi

echo "âœ… Git workflow validation passed"
EOF
chmod +x .git/hooks/pre-commit

# 2. Create MR template with proper targeting
mkdir -p .github
cat > .github/pull_request_template.md << 'EOF'
## Summary
<!-- 2-3 sentences describing what this MR accomplishes -->

## Changes Made
- ğŸ”§ **API**: <!-- API changes if any -->
- ğŸ—„ï¸ **Database**: <!-- Database changes if any -->
- ğŸ§ª **Tests**: <!-- Test additions/changes -->
- ğŸ”’ **Security**: <!-- Security improvements -->

## Testing Performed
- [ ] Unit tests: X/X passing (100%)
- [ ] Integration tests: X/X passing (100%)
- [ ] Playwright E2E: X/6 browsers passing
- [ ] Manual API testing: All endpoints verified
- [ ] Security scan: All issues resolved

## API Verification Examples
```bash
# Include cURL examples for any new/modified endpoints
```

## Related Jira Tickets
<!-- List PR003946-XX tickets addressed -->

## Pre-Review Checklist
- [ ] All comments resolved with documentation
- [ ] All inline comments resolved with documentation
- [ ] All quality gates passed
- [ ] All CodeQL issues addressed
- [ ] Self-review completed
- [ ] Session history documented in `/history/`

## Target Branch
**IMPORTANT**: This MR targets `dev` branch (not main/master)

## Deployment Notes
<!-- Any special deployment requirements -->
EOF

# 3. Create automated MR creation script
cat > scripts/create_mr.sh << 'EOF'
#!/bin/bash
set -e

current_branch=$(git branch --show-current)

# Validate we're on a feature branch
if [[ "$current_branch" == "main" || "$current_branch" == "master" || "$current_branch" == "dev" ]]; then
    echo "âŒ Cannot create MR from protected branch: $current_branch"
    echo "   Switch to a feature branch first"
    exit 1
fi

# Ensure we're up to date with dev
echo "ğŸ”„ Updating dev branch..."
git fetch origin dev
git rebase origin/dev

# Run quality checks
echo "ğŸ” Running quality checks..."
if command -v pytest >/dev/null 2>&1; then
    pytest --cov=openapi_server || echo "âš ï¸  Tests failed - continue with MR creation? (Ctrl+C to abort)"
fi

# Push branch
echo "ğŸ“¤ Pushing branch..."
git push -u origin "$current_branch"

# Create MR targeting dev
echo "ğŸ“ Creating MR targeting dev branch..."
gh pr create --base dev --fill

echo "âœ… MR created successfully targeting dev branch"
echo "   Add reviewer: gh pr edit \$(gh pr list --state open --head $current_branch --json number --jq '.[0].number') --add-reviewer Copilot"
EOF
chmod +x scripts/create_mr.sh
```

#### Step 4: Quality Gate Automation (QUALITY IMPACT)
**Problem**: CodeQL issues accumulate and block MRs
**Solution**: Proactive quality checking and automated fixes

```bash
# 1. Create comprehensive quality check script
cat > scripts/quality_gates.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸ” Running CMZ Quality Gates..."

# Function to run check and track results
run_check() {
    local check_name="$1"
    local command="$2"
    echo "ğŸ”„ Running $check_name..."

    if eval "$command"; then
        echo "âœ… $check_name: PASSED"
        return 0
    else
        echo "âŒ $check_name: FAILED"
        return 1
    fi
}

cd "$(dirname "$0")/.."
failed_checks=0

# Python import validation
if ! run_check "Python Import Validation" "cd backend/api/src/main/python && python -c 'from openapi_server.models import *; from openapi_server.impl import handlers'"; then
    ((failed_checks++))
fi

# Unit tests
if ! run_check "Unit Tests" "pytest tests/unit/ -v"; then
    ((failed_checks++))
fi

# Code formatting
if ! run_check "Code Formatting" "black --check backend/api/src/main/python/openapi_server/impl/"; then
    echo "   Run: black backend/api/src/main/python/openapi_server/impl/"
    ((failed_checks++))
fi

# Linting
if ! run_check "Code Linting" "flake8 backend/api/src/main/python/openapi_server/impl/"; then
    ((failed_checks++))
fi

# Security scanning (if bandit is available)
if command -v bandit >/dev/null 2>&1; then
    if ! run_check "Security Scan" "bandit -r backend/api/src/main/python/openapi_server/impl/ -ll"; then
        ((failed_checks++))
    fi
fi

# API health check (if running)
if curl -f http://localhost:8080/health >/dev/null 2>&1; then
    if ! run_check "API Health Check" "curl -f http://localhost:8080/health"; then
        ((failed_checks++))
    fi
else
    echo "âš ï¸  API not running - skipping health check"
fi

# Summary
echo "ğŸ“Š Quality Gates Summary:"
if [ $failed_checks -eq 0 ]; then
    echo "âœ… All quality gates passed!"
    exit 0
else
    echo "âŒ $failed_checks quality gate(s) failed"
    echo "   Fix issues before creating MR"
    exit 1
fi
EOF
chmod +x scripts/quality_gates.sh

# 2. Create automated fix script for common issues
cat > scripts/fix_common_issues.sh << 'EOF'
#!/bin/bash
echo "ğŸ”§ Fixing common CMZ development issues..."

cd "$(dirname "$0")/.."

# Fix import paths (common after OpenAPI regeneration)
echo "ğŸ”„ Fixing import paths..."
find backend/api/src/main/python/openapi_server/controllers -name "*.py" -exec sed -i '' 's/from openapi_server\.controllers\.impl/from openapi_server.impl/g' {} \;
find backend/api/src/main/python/openapi_server/controllers -name "*.py" -exec sed -i '' 's/from openapi_server\.controllers\.models/from openapi_server.models/g' {} \;
find backend/api/src/main/python/openapi_server -name "__main__.py" -exec sed -i '' 's/from  import encoder/from . import encoder/g' {} \;

# Format code
echo "ğŸ¨ Formatting code..."
if command -v black >/dev/null 2>&1; then
    black backend/api/src/main/python/openapi_server/impl/
fi

# Remove unused imports (basic cleanup)
echo "ğŸ§¹ Cleaning up imports..."
find backend/api/src/main/python/openapi_server/impl -name "*.py" -exec python3 -c "
import re
import sys
with open(sys.argv[1], 'r') as f:
    content = f.read()
# Remove duplicate imports
lines = content.split('\n')
seen_imports = set()
cleaned_lines = []
for line in lines:
    if line.strip().startswith('import ') or line.strip().startswith('from '):
        if line not in seen_imports:
            seen_imports.add(line)
            cleaned_lines.append(line)
    else:
        cleaned_lines.append(line)
with open(sys.argv[1], 'w') as f:
    f.write('\n'.join(cleaned_lines))
" {} \;

echo "âœ… Common issues fixed - run quality_gates.sh to validate"
EOF
chmod +x scripts/fix_common_issues.sh

# 3. Add Makefile targets for quality
cat >> Makefile << 'EOF'

# Quality gates
quality-check: ## Run all quality gates
	@scripts/quality_gates.sh

fix-common: ## Fix common development issues
	@scripts/fix_common_issues.sh

pre-mr: ## Prepare for MR creation (quality check + branch push)
	@scripts/quality_gates.sh && scripts/create_mr.sh
EOF
```

### Phase 3: Comprehensive Testing & Validation Infrastructure
**Use Sequential Reasoning for systematic before/after validation:**

#### Step 1: Pre-Infrastructure Baseline Assessment
**Use Sequential Reasoning to predict current failure patterns:**

```bash
# 1. Document current pain points with evidence
echo "ğŸ“Š Pre-Infrastructure Baseline Assessment" > infrastructure-validation-log.md
echo "=========================================" >> infrastructure-validation-log.md
echo "**Test Date**: $(date)" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# 2. Run validate-animal-config with failure prediction
echo "ğŸ” **PRE-FIX: validate-animal-config command**" >> infrastructure-validation-log.md
echo "**Predicted Failures**:" >> infrastructure-validation-log.md
echo "- Form validation: DOM element access across tabs" >> infrastructure-validation-log.md
echo "- API connectivity: Import path errors after regeneration" >> infrastructure-validation-log.md
echo "- Save functionality: Frontend-backend state mismatch" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# Run the actual validation (expect failures)
echo "**Actual Results**:" >> infrastructure-validation-log.md
if /validate-animal-config 2>&1 | tee -a infrastructure-validation-log.md; then
    echo "âŒ UNEXPECTED: validate-animal-config passed (should fail pre-fix)" >> infrastructure-validation-log.md
else
    echo "âœ… EXPECTED: validate-animal-config failed as predicted" >> infrastructure-validation-log.md
fi
echo "" >> infrastructure-validation-log.md

# 3. Run validate-animal-config-edit with failure prediction
echo "ğŸ” **PRE-FIX: validate-animal-config-edit command**" >> infrastructure-validation-log.md
echo "**Predicted Failures**:" >> infrastructure-validation-log.md
echo "- Modal opening: Controller import errors" >> infrastructure-validation-log.md
echo "- Form interaction: Tabbed interface validation failures" >> infrastructure-validation-log.md
echo "- Save operation: Backend handler routing issues" >> infrastructure-validation-log.md
echo "- Data persistence: DynamoDB connection through broken API" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# Run the actual validation (expect failures)
echo "**Actual Results**:" >> infrastructure-validation-log.md
if /validate-animal-config-edit 2>&1 | tee -a infrastructure-validation-log.md; then
    echo "âŒ UNEXPECTED: validate-animal-config-edit passed (should fail pre-fix)" >> infrastructure-validation-log.md
else
    echo "âœ… EXPECTED: validate-animal-config-edit failed as predicted" >> infrastructure-validation-log.md
fi
echo "" >> infrastructure-validation-log.md

# 4. TDD Test Prediction and Execution
echo "ğŸ§ª **PRE-FIX: TDD Test Suite Predictions**" >> infrastructure-validation-log.md
echo "**Unit Test Predictions**:" >> infrastructure-validation-log.md
echo "- Import tests: FAIL (import path errors)" >> infrastructure-validation-log.md
echo "- Controller tests: FAIL (handler connection issues)" >> infrastructure-validation-log.md
echo "- Model tests: FAIL (generation errors)" >> infrastructure-validation-log.md
echo "**Integration Test Predictions**:" >> infrastructure-validation-log.md
echo "- API endpoint tests: FAIL (501 Not Implemented responses)" >> infrastructure-validation-log.md
echo "- Form validation tests: FAIL (DOM access issues)" >> infrastructure-validation-log.md
echo "- E2E tests: FAIL (service startup issues)" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# Run actual tests with prediction validation
echo "**Actual Unit Test Results**:" >> infrastructure-validation-log.md
pytest tests/unit/test_openapi_endpoints.py -v 2>&1 | tee -a infrastructure-validation-log.md || echo "âœ… EXPECTED: Unit tests failed as predicted" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

echo "**Actual Integration Test Results**:" >> infrastructure-validation-log.md
pytest tests/integration/test_api_validation_epic.py -v 2>&1 | tee -a infrastructure-validation-log.md || echo "âœ… EXPECTED: Integration tests failed as predicted" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md
```

#### Step 2: Infrastructure Implementation (Previous Steps)
**Apply all infrastructure improvements from Steps 1-4 above**

#### Step 3: Post-Infrastructure Validation with Success Predictions
**Use Sequential Reasoning to predict improvement patterns:**

```bash
# 1. Predict post-fix success patterns
echo "ğŸ¯ **POST-FIX: Infrastructure Improvement Predictions**" >> infrastructure-validation-log.md
echo "=================================================" >> infrastructure-validation-log.md
echo "**Expected Improvements**:" >> infrastructure-validation-log.md
echo "- OpenAPI regeneration: Zero import errors" >> infrastructure-validation-log.md
echo "- Controller connections: Automatic handler routing" >> infrastructure-validation-log.md
echo "- Environment startup: One-command full system launch" >> infrastructure-validation-log.md
echo "- Quality gates: Proactive issue detection" >> infrastructure-validation-log.md
echo "- Git workflow: Enforced branch targeting" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# 2. Test template fix effectiveness
echo "ğŸ”§ **POST-FIX: Template Fix Validation**" >> infrastructure-validation-log.md
echo "**Predicted Success**: OpenAPI regeneration with zero manual fixes required" >> infrastructure-validation-log.md
make generate-api 2>&1 | tee -a infrastructure-validation-log.md
python scripts/post_openapi_generation.py 2>&1 | tee -a infrastructure-validation-log.md

# Validate import resolution
echo "**Import Validation Results**:" >> infrastructure-validation-log.md
if python -c "from openapi_server.models import *; from openapi_server.impl import handlers" 2>&1 | tee -a infrastructure-validation-log.md; then
    echo "âœ… SUCCESS: Python imports work without manual fixes" >> infrastructure-validation-log.md
else
    echo "âŒ FAILURE: Import issues persist - investigate template configuration" >> infrastructure-validation-log.md
fi
echo "" >> infrastructure-validation-log.md

# 3. Test environment automation effectiveness
echo "ğŸš€ **POST-FIX: Environment Automation Validation**" >> infrastructure-validation-log.md
echo "**Predicted Success**: One-command startup with health validation" >> infrastructure-validation-log.md
make stop-dev 2>&1 | tee -a infrastructure-validation-log.md
make start-dev 2>&1 | tee -a infrastructure-validation-log.md
make health-check 2>&1 | tee -a infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# 4. Re-run validate-animal-config with success prediction
echo "ğŸ” **POST-FIX: validate-animal-config command**" >> infrastructure-validation-log.md
echo "**Predicted Improvements**:" >> infrastructure-validation-log.md
echo "- Form validation: React state management instead of DOM access" >> infrastructure-validation-log.md
echo "- API connectivity: Fixed import paths and handler routing" >> infrastructure-validation-log.md
echo "- Save functionality: Controlled components with proper state flow" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

echo "**Actual Results**:" >> infrastructure-validation-log.md
if /validate-animal-config 2>&1 | tee -a infrastructure-validation-log.md; then
    echo "âœ… SUCCESS: validate-animal-config now passes" >> infrastructure-validation-log.md
else
    echo "âŒ PARTIAL: Some issues remain - analyze specific failures" >> infrastructure-validation-log.md
fi
echo "" >> infrastructure-validation-log.md

# 5. Re-run validate-animal-config-edit with success prediction
echo "ğŸ” **POST-FIX: validate-animal-config-edit command**" >> infrastructure-validation-log.md
echo "**Predicted Improvements**:" >> infrastructure-validation-log.md
echo "- Modal opening: Automated controller-handler connections" >> infrastructure-validation-log.md
echo "- Form interaction: Tab-independent validation system" >> infrastructure-validation-log.md
echo "- Save operation: Generic handler routing eliminates 501 errors" >> infrastructure-validation-log.md
echo "- Data persistence: Stable API endpoints with proper error handling" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

echo "**Actual Results**:" >> infrastructure-validation-log.md
if /validate-animal-config-edit 2>&1 | tee -a infrastructure-validation-log.md; then
    echo "âœ… SUCCESS: validate-animal-config-edit now passes" >> infrastructure-validation-log.md
else
    echo "âŒ PARTIAL: Some issues remain - analyze specific failures" >> infrastructure-validation-log.md
fi
echo "" >> infrastructure-validation-log.md

# 6. TDD Test Suite Re-validation with Success Predictions
echo "ğŸ§ª **POST-FIX: TDD Test Suite Validation**" >> infrastructure-validation-log.md
echo "**Unit Test Success Predictions**:" >> infrastructure-validation-log.md
echo "- Import tests: PASS (fixed import paths)" >> infrastructure-validation-log.md
echo "- Controller tests: PASS (automatic handler connections)" >> infrastructure-validation-log.md
echo "- Model tests: PASS (clean generation)" >> infrastructure-validation-log.md
echo "**Integration Test Success Predictions**:" >> infrastructure-validation-log.md
echo "- API endpoint tests: PASS (working handler routing)" >> infrastructure-validation-log.md
echo "- Form validation tests: PASS (React state management)" >> infrastructure-validation-log.md
echo "- E2E tests: PASS (reliable environment startup)" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# Run actual tests with improvement validation
echo "**Actual Unit Test Results**:" >> infrastructure-validation-log.md
if pytest tests/unit/test_openapi_endpoints.py -v 2>&1 | tee -a infrastructure-validation-log.md; then
    echo "âœ… SUCCESS: Unit tests now pass" >> infrastructure-validation-log.md
else
    echo "âŒ PARTIAL: Some unit tests still failing - analyze remaining issues" >> infrastructure-validation-log.md
fi
echo "" >> infrastructure-validation-log.md

echo "**Actual Integration Test Results**:" >> infrastructure-validation-log.md
if pytest tests/integration/test_api_validation_epic.py -v 2>&1 | tee -a infrastructure-validation-log.md; then
    echo "âœ… SUCCESS: Integration tests now pass" >> infrastructure-validation-log.md
else
    echo "âŒ PARTIAL: Some integration tests still failing - analyze remaining issues" >> infrastructure-validation-log.md
fi
echo "" >> infrastructure-validation-log.md

# 7. E2E Testing with Environment Validation
echo "ğŸ­ **POST-FIX: End-to-End Testing**" >> infrastructure-validation-log.md
echo "**Predicted Success**: Playwright tests pass with stable environment" >> infrastructure-validation-log.md
cd backend/api/src/main/python/tests/playwright
./run-step1-validation.sh 2>&1 | tee -a ../../../../../../infrastructure-validation-log.md
echo "" >> ../../../../../../infrastructure-validation-log.md

# If Step 1 passes, run full suite
if [ $? -eq 0 ]; then
    echo "âœ… Step 1 validation passed - running full Playwright suite" >> ../../../../../../infrastructure-validation-log.md
    FRONTEND_URL=http://localhost:3001 npx playwright test --config config/playwright.config.js --reporter=line 2>&1 | tee -a ../../../../../../infrastructure-validation-log.md
else
    echo "âŒ Step 1 validation failed - investigate authentication/environment issues" >> ../../../../../../infrastructure-validation-log.md
fi

cd ../../../../../../
```

#### Step 4: Comprehensive Impact Analysis
**Use Sequential Reasoning to analyze improvement effectiveness:**

```bash
# Generate final impact assessment
echo "ğŸ“ˆ **INFRASTRUCTURE IMPACT ANALYSIS**" >> infrastructure-validation-log.md
echo "===================================" >> infrastructure-validation-log.md
echo "**Test Date**: $(date)" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

# Calculate improvement metrics
echo "**Validation Command Improvements**:" >> infrastructure-validation-log.md
echo "- validate-animal-config: $(grep -c 'SUCCESS.*validate-animal-config' infrastructure-validation-log.md)/1 commands now passing" >> infrastructure-validation-log.md
echo "- validate-animal-config-edit: $(grep -c 'SUCCESS.*validate-animal-config-edit' infrastructure-validation-log.md)/1 commands now passing" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

echo "**Test Suite Improvements**:" >> infrastructure-validation-log.md
echo "- Unit tests: $(grep -c 'SUCCESS.*Unit tests' infrastructure-validation-log.md)/1 test suites now passing" >> infrastructure-validation-log.md
echo "- Integration tests: $(grep -c 'SUCCESS.*Integration tests' infrastructure-validation-log.md)/1 test suites now passing" >> infrastructure-validation-log.md
echo "- E2E tests: $(grep -c 'Step 1 validation passed' infrastructure-validation-log.md)/1 test suites now passing" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

echo "**Infrastructure Components**:" >> infrastructure-validation-log.md
echo "- OpenAPI templates: $(grep -c 'SUCCESS.*Python imports' infrastructure-validation-log.md)/1 working without manual fixes" >> infrastructure-validation-log.md
echo "- Environment automation: $(grep -c 'healthy' infrastructure-validation-log.md) services automatically started and validated" >> infrastructure-validation-log.md
echo "- Quality gates: $(ls scripts/quality_gates.sh >/dev/null 2>&1 && echo "1" || echo "0")/1 automated quality validation implemented" >> infrastructure-validation-log.md
echo "- Git workflow: $(ls .git/hooks/pre-commit >/dev/null 2>&1 && echo "1" || echo "0")/1 branch protection and automation implemented" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

echo "**Prediction Accuracy Assessment**:" >> infrastructure-validation-log.md
echo "- Pre-fix failure predictions: $(grep -c 'EXPECTED.*failed as predicted' infrastructure-validation-log.md) accurate predictions" >> infrastructure-validation-log.md
echo "- Post-fix success predictions: $(grep -c 'SUCCESS' infrastructure-validation-log.md) successful improvements" >> infrastructure-validation-log.md
echo "- Remaining issues: $(grep -c 'PARTIAL\|FAILURE' infrastructure-validation-log.md) items requiring further analysis" >> infrastructure-validation-log.md
echo "" >> infrastructure-validation-log.md

echo "**Next Steps for Remaining Issues**:" >> infrastructure-validation-log.md
if grep -q 'PARTIAL\|FAILURE' infrastructure-validation-log.md; then
    echo "1. Analyze specific failure patterns in test output above" >> infrastructure-validation-log.md
    echo "2. Apply targeted fixes for remaining edge cases" >> infrastructure-validation-log.md
    echo "3. Re-run validation cycle for affected components" >> infrastructure-validation-log.md
    echo "4. Update infrastructure scripts based on lessons learned" >> infrastructure-validation-log.md
else
    echo "ğŸ‰ All predicted improvements successful - infrastructure hardening complete!" >> infrastructure-validation-log.md
    echo "1. Document lessons learned in team knowledge base" >> infrastructure-validation-log.md
    echo "2. Train team on new infrastructure commands" >> infrastructure-validation-log.md
    echo "3. Monitor usage and gather feedback for continuous improvement" >> infrastructure-validation-log.md
fi

echo "" >> infrastructure-validation-log.md
echo "**Full validation log available**: infrastructure-validation-log.md" >> infrastructure-validation-log.md
```

### Phase 4: Documentation, Monitoring & Completion Notification
**Create comprehensive monitoring and documentation, then notify completion:**

#### Step 1: Send Completion Notification
**Notify user of infrastructure hardening completion:**

```bash
# Prepare completion notification with results summary
COMPLETION_TIME=$(date '+%Y-%m-%d %H:%M:%S')
VALIDATION_RESULTS=$(grep -c 'SUCCESS' infrastructure-validation-log.md 2>/dev/null || echo "0")
REMAINING_ISSUES=$(grep -c 'PARTIAL\|FAILURE' infrastructure-validation-log.md 2>/dev/null || echo "0")

# Create notification message
NOTIFICATION_MESSAGE="ğŸ‰ CMZ Infrastructure Hardening Complete!

ğŸ“Š Results Summary:
- Completion Time: $COMPLETION_TIME
- Successful Validations: $VALIDATION_RESULTS
- Remaining Issues: $REMAINING_ISSUES
- Full Log: infrastructure-validation-log.md

ğŸ”§ Infrastructure Components Deployed:
- OpenAPI Template Fix: Eliminates controller-handler mismatches
- Environment Automation: One-command startup with health checks
- Quality Gates: Proactive issue detection and fixes
- Git Workflow: Enforced branch targeting and validation

ğŸ“ˆ Expected Impact:
- 100% elimination of OpenAPI regeneration issues
- 80% reduction in environment startup time
- 95% reduction in git workflow errors
- 70% reduction in quality gate failures

Ready for team adoption and training! ğŸš€"

# Try SMS notification via ClickSend first
echo "ğŸ“± Attempting SMS notification via ClickSend..."
SMS_RESULT=$(curl -X POST https://rest.clicksend.com/v3/sms/send \
  -H "Content-Type: application/json" \
  -u "stegbk@hotmail.com:A6FA27BC-844B-254B-DD06-85DACBB14F96" \
  -d "{
    \"messages\": [
      {
        \"to\": \"+12063070100\",
        \"body\": \"CMZ Infrastructure Hardening Complete! $VALIDATION_RESULTS successes, $REMAINING_ISSUES remaining issues. Check infrastructure-validation-log.md for details.\",
        \"from\": \"CMZ\"
      }
    ]
  }" 2>/dev/null)

# Check if SMS was successful
if echo "$SMS_RESULT" | grep -q '"response_code":"SUCCESS"' && echo "$SMS_RESULT" | grep -q '"status":"QUEUED"'; then
    echo "âœ… SMS notification sent successfully via ClickSend"
    echo "ğŸ“± SMS Status: $(echo "$SMS_RESULT" | grep -o '"status":"[^"]*"')"

    # Log SMS success
    echo "**Completion Notification**: SMS sent successfully to +12063070100" >> infrastructure-validation-log.md

elif echo "$SMS_RESULT" | grep -q '"status":"COUNTRY_NOT_ENABLED"'; then
    echo "âš ï¸  SMS blocked: COUNTRY_NOT_ENABLED - falling back to email"
    echo "ğŸ’¡ To fix: Register US number in ClickSend dashboard (SMS > Countries)"

    # Send email notification instead
    echo "ğŸ“§ Sending email notification to stegbk@hotmail.com..."

    # Create email content
    EMAIL_SUBJECT="CMZ Infrastructure Hardening Complete - $(date '+%Y-%m-%d %H:%M')"
    EMAIL_BODY="$NOTIFICATION_MESSAGE

ğŸ“ Detailed Results:
$(tail -20 infrastructure-validation-log.md 2>/dev/null || echo 'Log file not available')

This automated notification was sent because SMS delivery failed due to COUNTRY_NOT_ENABLED status in ClickSend.
To enable SMS notifications, register a US number in the ClickSend dashboard under SMS > Countries.

Best regards,
CMZ Infrastructure Automation"

    # Try multiple email sending methods
    if command -v sendmail >/dev/null 2>&1; then
        # Method 1: sendmail (if available)
        {
            echo "To: stegbk@hotmail.com"
            echo "Subject: $EMAIL_SUBJECT"
            echo "Content-Type: text/plain; charset=UTF-8"
            echo ""
            echo "$EMAIL_BODY"
        } | sendmail stegbk@hotmail.com
        echo "âœ… Email sent via sendmail"

    elif command -v mail >/dev/null 2>&1; then
        # Method 2: mail command (if available)
        echo "$EMAIL_BODY" | mail -s "$EMAIL_SUBJECT" stegbk@hotmail.com
        echo "âœ… Email sent via mail command"

    elif command -v curl >/dev/null 2>&1; then
        # Method 3: Generic SMTP via curl (basic attempt)
        echo "âš ï¸  No sendmail/mail available - logging email content for manual review"
        echo "ğŸ“§ Email would be sent to: stegbk@hotmail.com"
        echo "ğŸ“§ Subject: $EMAIL_SUBJECT"
        echo "ğŸ“§ Body: $EMAIL_BODY"

        # Save email to file for manual review
        {
            echo "To: stegbk@hotmail.com"
            echo "Subject: $EMAIL_SUBJECT"
            echo "Date: $(date)"
            echo ""
            echo "$EMAIL_BODY"
        } > cmz-infrastructure-completion-email.txt

        echo "ğŸ“„ Email content saved to: cmz-infrastructure-completion-email.txt"
        echo "   Please manually send or configure SMTP for automated delivery"

    else
        echo "âŒ No email sending methods available"
        echo "ğŸ“„ Notification content saved to: cmz-infrastructure-completion-email.txt"
        {
            echo "NOTIFICATION: $EMAIL_SUBJECT"
            echo "============================================"
            echo "$EMAIL_BODY"
        } > cmz-infrastructure-completion-email.txt
    fi

    # Log email fallback
    echo "**Completion Notification**: Email fallback used due to SMS COUNTRY_NOT_ENABLED" >> infrastructure-validation-log.md

else
    echo "âŒ SMS notification failed - ClickSend error"
    echo "ğŸ” SMS Response: $SMS_RESULT"

    # Try email as fallback for any SMS failure
    echo "ğŸ“§ Attempting email notification as fallback..."

    EMAIL_SUBJECT="CMZ Infrastructure Hardening Complete - $(date '+%Y-%m-%d %H:%M')"
    EMAIL_BODY="$NOTIFICATION_MESSAGE

ğŸ“ Note: This email was sent as a fallback because SMS notification failed.
ClickSend Response: $SMS_RESULT

$(tail -20 infrastructure-validation-log.md 2>/dev/null || echo 'Log file not available')"

    # Save notification regardless of email capability
    {
        echo "To: stegbk@hotmail.com"
        echo "Subject: $EMAIL_SUBJECT"
        echo "Date: $(date)"
        echo ""
        echo "$EMAIL_BODY"
    } > cmz-infrastructure-completion-email.txt

    echo "ğŸ“„ Notification saved to: cmz-infrastructure-completion-email.txt"
    echo "   Review and send manually if automated email fails"

    # Log SMS failure
    echo "**Completion Notification**: SMS failed, email fallback attempted" >> infrastructure-validation-log.md
fi

echo ""
echo "ğŸ‰ CMZ Infrastructure Hardening Complete!"
echo "ğŸ“Š Validation Results: $VALIDATION_RESULTS successes, $REMAINING_ISSUES remaining issues"
echo "ğŸ“„ Full log: infrastructure-validation-log.md"
echo "ğŸ“± Notification status logged above"
```

#### Step 2: Create Infrastructure Status Dashboard
```bash
# Add status command to Makefile
cat >> Makefile << 'EOF'

status: ## Show complete system status
	@echo "ğŸ—ï¸  CMZ Infrastructure Status"
	@echo "================================"
	@echo "Services:"
	@curl -f http://localhost:8080/health >/dev/null 2>&1 && echo "  âœ… Backend API (8080)" || echo "  âŒ Backend API (8080)"
	@curl -f http://localhost:3000 >/dev/null 2>&1 && echo "  âœ… Frontend (3000)" || echo "  âŒ Frontend (3000)"
	@curl -f http://localhost:3001 >/dev/null 2>&1 && echo "  âœ… Frontend (3001)" || echo "  âŒ Frontend (3001)"
	@aws dynamodb list-tables --region us-west-2 --profile cmz >/dev/null 2>&1 && echo "  âœ… DynamoDB" || echo "  âŒ DynamoDB"
	@echo ""
	@echo "Git Status:"
	@echo "  Branch: $$(git branch --show-current)"
	@echo "  Status: $$(git status --porcelain | wc -l) uncommitted changes"
	@echo ""
	@echo "Quality Status:"
	@python -c "from openapi_server.models import *" 2>/dev/null && echo "  âœ… Python imports" || echo "  âŒ Python imports"
	@black --check backend/api/src/main/python/openapi_server/impl/ >/dev/null 2>&1 && echo "  âœ… Code formatting" || echo "  âŒ Code formatting"
EOF
```

#### Step 2: Update Project Documentation
```bash
# Update CLAUDE.md with new infrastructure commands
cat >> ../repositories/CMZ-chatbots/CLAUDE.md << 'EOF'

## Infrastructure Hardening (Added 2025-01-14)

### Automated Environment Management
- `make start-dev` - Start complete development environment with health checks
- `make stop-dev` - Stop all development services
- `make status` - Show complete system status
- `make health-check` - Verify all services are healthy

### Quality Automation
- `make quality-check` - Run all quality gates before MR creation
- `make fix-common` - Automatically fix common development issues
- `make pre-mr` - Complete pre-MR workflow (quality + branch creation)

### Infrastructure Commands
- `scripts/start_development_environment.sh` - Comprehensive startup with validation
- `scripts/quality_gates.sh` - Complete quality validation
- `scripts/fix_common_issues.sh` - Automated issue resolution
- `scripts/create_mr.sh` - Proper MR creation targeting dev branch

### Key Improvements
- **OpenAPI Template Fix**: Eliminates controller-implementation mismatches permanently
- **Automated Environment**: No more manual service startup or port conflicts
- **Quality Gates**: Proactive issue detection and automated fixes
- **Git Workflow**: Enforced branch targeting and naming conventions
EOF
```

## Integration Points
- **Existing Make Commands**: Extends current Makefile without breaking existing workflows
- **Docker Integration**: Works with existing container infrastructure
- **AWS/DynamoDB**: Maintains current AWS profile and region configurations
- **Git Workflow**: Enforces existing feature branch patterns
- **Quality Standards**: Integrates with existing testing and security requirements

## Quality Gates
### Mandatory Validation Before Completion
- [ ] OpenAPI template fix tested with full regeneration cycle
- [ ] Automated environment startup tested on clean system
- [ ] Git workflow enforcement tested with branch protection
- [ ] Quality gates tested with intentional failures
- [ ] All scripts have proper error handling and validation
- [ ] Documentation updated in CLAUDE.md
- [ ] Makefile targets added and tested

### System Integration Validation
- [ ] New infrastructure works with existing Docker workflow
- [ ] Quality automation integrates with current testing patterns
- [ ] Git enforcement doesn't break existing development practices
- [ ] Environment automation respects existing AWS configurations

## Success Criteria
1. **Zero OpenAPI Regeneration Issues**: Template fix eliminates controller-implementation mismatches
2. **Automated Environment Startup**: One command starts entire development environment
3. **Enforced Git Workflow**: Impossible to create MRs targeting wrong branches
4. **Proactive Quality**: Issues caught and fixed before they block development
5. **Comprehensive Monitoring**: Clear visibility into system health and status
6. **Team Adoption**: All developers use new infrastructure commands consistently

## Expected Impact Reduction
- **OpenAPI Issues**: 100% elimination (architectural fix)
- **Environment Startup Time**: 80% reduction (automated vs manual)
- **Git Workflow Errors**: 95% reduction (enforced validation)
- **Quality Gate Failures**: 70% reduction (proactive detection)
- **Development Friction**: 60% overall reduction in time spent on recurring issues

## References
- `SYSTEMATIC-CMZ-INFRASTRUCTURE-HARDENING-ADVICE.md` - Implementation best practices and troubleshooting
- Existing CMZ development documentation for integration patterns
- OpenAPI generator documentation for template customization